{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation\n",
        "Pre-process the data for the machine learning model:\n",
        "\n",
        "1. Define variables\n",
        "2. Load the datasets\n",
        "3. Convert string to list and remove punctuations from text\n",
        "4. Store datasets\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import os\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType, StringType, ArrayType, DoubleType, MapType\n",
        "from pyspark.sql import functions as F\n",
        "import re\n",
        "\n",
        "spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define variables (input folder, datasets, ....)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Define adls path and datset filename\n",
        "account_name = \"\"\n",
        "container = \"cms\"\n",
        "\n",
        "input_train_folder = 'MINDsmall_train'\n",
        "input_dev_folder = 'MINDsmall_dev'\n",
        "input_test_folder = 'MINDlarge_test'\n",
        "\n",
        "ds_name_activity = 'behaviors.tsv'\n",
        "df_name_news = 'news.tsv'\n",
        "\n",
        "# Azure Storage path\n",
        "adls_path = \"abfss://%s@%s.dfs.core.windows.net/MicrosoftNewsDataset/\" % (container, account_name)\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the news and behavior datasets\n",
        "The news dataset contains all the metadata of an article and the behavior dataset contains the activity of the users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Load data into Spark Table\n",
        "\n",
        "\n",
        "#### Behavior dataset\n",
        "# Define schema \n",
        "schema = StructType([\n",
        "    StructField(\"Impression_ID\", IntegerType(), True),\n",
        "    StructField(\"User_ID\", StringType(), True),\n",
        "    StructField(\"Time\", StringType(), True),\n",
        "    StructField(\"History\", StringType(), True),\n",
        "    StructField(\"Impressions\", StringType(), True)])\n",
        "\n",
        "# load train dataset\n",
        "df_activity_train = spark.read.csv(\n",
        "    os.path.join(adls_path,input_train_folder,ds_name_activity),\n",
        "    inferSchema=True,\n",
        "    sep='\\t', \n",
        "    header=False,\n",
        "    schema=schema\n",
        "    )\n",
        "\n",
        "# load dev dataset\n",
        "df_activity_dev = spark.read.csv(\n",
        "    os.path.join(adls_path,input_dev_folder,ds_name_activity),\n",
        "    inferSchema=True,\n",
        "    sep='\\t', \n",
        "    header=False,\n",
        "    schema=schema\n",
        "    )\n",
        "\n",
        "# load test dataset\n",
        "df_activity_test = spark.read.csv(\n",
        "    os.path.join(adls_path,input_test_folder,ds_name_activity),\n",
        "    inferSchema=True,\n",
        "    sep='\\t', \n",
        "    header=False,\n",
        "    schema=schema\n",
        "    )\n",
        "\n",
        "# keep only a fraction of the total test dataset \n",
        "fraction = df_activity_train.count() / df_activity_test.count()\n",
        "df_activity_test = df_activity_test.sample(fraction=fraction, seed=2020)\n",
        "\n",
        "# Keep link to dataset in dictionary\n",
        "dict_data = {'train': df_activity_train, 'test': df_activity_test, 'dev': df_activity_dev}\n",
        "\n",
        "###### News Dataset\n",
        "# Define Schema\n",
        "schema = StructType([\n",
        "    StructField(\"News_ID\", StringType(), True),\n",
        "    StructField(\"Category\", StringType(), True),\n",
        "    StructField(\"SubCategory\", StringType(), True),\n",
        "    StructField(\"Title\", StringType(), True),\n",
        "    StructField(\"Abstract\", StringType(), True),\n",
        "    StructField(\"URL\", StringType(), True),\n",
        "    StructField(\"Title_Entities\", StringType(), True),\n",
        "    StructField(\"Abstract_Entities\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "# Load data into Spark Table\n",
        "df_news_train = spark.read.csv(\n",
        "    os.path.join(adls_path,input_train_folder,df_name_news),\n",
        "    # inferSchema=True,\n",
        "    sep='\\t', \n",
        "    header=False,\n",
        "    schema=schema\n",
        "    )\n",
        "\n",
        "# load dev dataset\n",
        "df_news_dev = spark.read.csv(\n",
        "    os.path.join(adls_path,input_dev_folder,df_name_news),\n",
        "    # inferSchema=True,\n",
        "    sep='\\t', \n",
        "    header=False,\n",
        "    schema=schema\n",
        "    )\n",
        "\n",
        "# load test dataset\n",
        "df_news_test = spark.read.csv(\n",
        "    os.path.join(adls_path,input_test_folder,df_name_news),\n",
        "    # inferSchema=True,\n",
        "    sep='\\t', \n",
        "    header=False,\n",
        "    schema=schema\n",
        "    )\n",
        "    \n",
        "# Keep link to dataset in dictionary\n",
        "dict_news = {'train': df_news_train, 'test': df_news_test, 'dev': df_news_dev}\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process datasets\n",
        "\n",
        "The code removes all punctuation from the columns containing textual information like the abstract. Moreover, it converts the cleaned texted into a list of strings. This will facilitate the convertion of strings to integers as an ML model can only treat numerical data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# helper function to remove punctuations\n",
        "def removePunctuation(text):\n",
        "    if text is not None:\n",
        "        text=text.lower().strip()\n",
        "        text=re.sub(\"[^0-9a-zA-Z ]\",\"\", text)\n",
        "        return text\n",
        "    return 'NoInfo'\n",
        "\n",
        "# helper function\n",
        "# convert a string list to actual list\n",
        "def convert_string_to_list(x):\n",
        "    if x is not None:\n",
        "        lst = x.split(' ')\n",
        "        return lst\n",
        "    return ['NoInfo']\n",
        "\n",
        "#Define user-define-function to apply on a spark dataframe\n",
        "udf_punc_remover = F.udf(lambda row: removePunctuation(row),StringType())\n",
        "udf_csl = F.udf(lambda row: convert_string_to_list(row),ArrayType(StringType()))\n",
        "\n",
        "#Define dictionary of results datasets\n",
        "results = {'train': None,'dev': None,'test':None}\n",
        "\n",
        "# Loop over the train, dev, test dataset to apply preprocessing\n",
        "for key, df_activity in dict_data.items():\n",
        "\n",
        "    # Convert string of past articles to list\n",
        "    df_preprocess = df_activity.withColumn('Impressions',udf_csl('Impressions'))\n",
        "    df_preprocess = df_preprocess.withColumn('History',udf_csl('History'))\n",
        "\n",
        "    # Extract list of past articles into a table of 1 User - 1 article\n",
        "    df_preprocess = df_preprocess.withColumn('flat_impressions',F.explode('Impressions'))\n",
        "\n",
        "    # Extract News ID\n",
        "    df_preprocess = df_preprocess.withColumn('News_ID',F.split(F.col(\"flat_impressions\"), \"-\").getItem(0))\n",
        "    if key != 'test':\n",
        "        # Extract target variables from news (whether the article was clicked on or not)\n",
        "        df_preprocess = df_preprocess.withColumn('Clicked',F.split(F.col(\"flat_impressions\"), \"-\").getItem(1).cast(IntegerType()))\n",
        "    \n",
        "    # drop temporary column\n",
        "    df_preprocess = df_preprocess.drop('flat_impressions')\n",
        "\n",
        "    # Join activity dataset with news metadata dataset\n",
        "    df_preprocess = df_preprocess.join(dict_news[key],on=['News_ID'],how='inner')\n",
        "\n",
        "    # Remove strings from IDs and convert them to integers\n",
        "    df_preprocess = df_preprocess.withColumn('News_ID',F.translate(\"News_ID\", \"N\", \"\").cast(IntegerType()))\n",
        "    df_preprocess = df_preprocess.withColumn('User_ID',F.translate(\"User_ID\", \"U\", \"\").cast(IntegerType()))\n",
        "\n",
        "    # Remove punctuations from title and abastract\n",
        "    df_preprocess = df_preprocess.withColumn(\"Title\", udf_punc_remover(\"Title\"))\n",
        "    df_preprocess = df_preprocess.withColumn(\"Abstract\", udf_punc_remover(\"Abstract\"))\n",
        "\n",
        "    #Drop any rows that contains null information\n",
        "    df_preprocess = df_preprocess.dropna()\n",
        "\n",
        "    # Keep results in dictionary\n",
        "    results[key] = df_preprocess\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Store datasets\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Write dataset to spark table\n",
        "results['train'].write.mode('overwrite').saveAsTable('default.ActivityTrain')\n",
        "results['test'].write.mode('overwrite').saveAsTable('default.ActivityTest')\n",
        "results['dev'].write.mode('overwrite').saveAsTable('default.ActivityDev')\n",
        ""
      ]
    }
  ]
}
