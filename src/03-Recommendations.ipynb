{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copyright (c) Microsoft Corporation.\n",
        "# Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recommendations\n",
        "\n",
        "Load the test datasets and apply the model to it. The resulting dataset contains two columns indicating whether the user will click on an article and the corresponding probability.\n",
        "\n",
        "1. Define variables\n",
        "2. Load the datasets, feature processor and model\n",
        "3. Apply model to dataset\n",
        "4. Cleanup results and store model\n",
        "5. Sample queries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.ml import PipelineModel, Pipeline\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "from mmlspark.lightgbm import LightGBMClassifier, LightGBMClassificationModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define variables \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#define general variables\n",
        "name_dataset_recommendation =  'default.Recommendations'\n",
        "model_name = \"news_recommendation_model.mml\"\n",
        "feature_processor_name = 'feature_proprecssor.mml'\n",
        "col_user = 'User_ID'\n",
        "col_item = 'Article_ID'\n",
        "dataset_test = 'default.activitytest'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load dataset, feature processor and model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Read testset\n",
        "df_test  = spark.read.table(dataset_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Load Model\n",
        "model = LightGBMClassificationModel.load(model_name)\n",
        "\n",
        "# Load Feature processor\n",
        "feature_processor = PipelineModel.load(feature_processor_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Apply feature processor and model to dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Apply feature processor to test data\n",
        "df_feature = feature_processor.transform(df_test)\n",
        "\n",
        "# Apply model to feature data\n",
        "df_recommendations = model.transform(df_feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean up results\n",
        "\n",
        "For the probabilities, the model returns a column with lists containing the probability of clicking and no-clicking on an article. In this case, the probability of clicking is extracted and stored in a new column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#post-processing\n",
        "udf_prob = F.udf(lambda x: float(x[1]), T.FloatType())\n",
        "df_output = df_recommendations.select(\n",
        "    'User_ID','News_ID','Category','Subcategory','Title','Abstract',\n",
        "    F.col('prediction').alias('ClickPrediction'), \n",
        "    udf_prob('probability').alias('ClickProbability')\n",
        "    )\n",
        "    \n",
        "df_output.write.mode('overwrite').saveAsTable('default.userRecommendations')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# get the user activity\n",
        "df_all_act = spark.sql('''\n",
        "select User_id, News_id, Category, SubCategory, Title, Abstract, to_timestamp(Time, \"MM/dd/yyyy hh:mm:ss aa\") as ActivityTime from default.ActivityTrain \n",
        "where User_id in (select distinct User_id from default.ActivityTest)\n",
        "''')\n",
        "df_all_act.write.mode('overwrite').saveAsTable('default.userAllActivity')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# get the user past activity and recommendations into one table\n",
        "df_all_act = spark.sql('''(select 'History' as ActType, User_id, News_id, Category, SubCategory, Title, \n",
        "Abstract, 0 as ClickPrediction, 0 as ClickProbability, ActivityTime from default.userAllActivity)\n",
        "UNION ALL\n",
        "(select 'Recommendations' as ActType, User_id, News_id, Category, SubCategory, Title, Abstract, \n",
        "ClickPrediction,ClickProbability, '9999-12-31 24:59:59' as ActivityTime from default.userRecommendations\n",
        "where user_id in (select distinct User_id from default.userAllActivity))\n",
        "''')\n",
        "\n",
        "df_all_act.write.mode('overwrite').saveAsTable('default.user_History_Recommendations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Sample Queries\r\n",
        "1. Count total recommendations\r\n",
        "2. Count unique users with recommendations\r\n",
        "3. Recommendations for a specific user and category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "sparksql"
        }
      },
      "source": [
        "%%sql\n",
        "\n",
        "select count(*) from default.user_History_Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "sparksql"
        }
      },
      "source": [
        "%%sql\n",
        "\n",
        "select count(distinct User_id) from default.user_History_Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        }
      },
      "source": [
        "%%sql \n",
        "Select * from default.user_History_Recommendations where User_ID = '66428' and Category = 'news' order by ActType, ClickProbability desc "
      ]
    }
  ]
}